{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e27d66",
   "metadata": {},
   "source": [
    "Department of Computer Engineering\n",
    " University of Peradeniya\n",
    " CO544: Machine Learning and Data Mining\n",
    " Lab 05: Text Classification and Performance Analysis\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea746608",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013ab13",
   "metadata": {},
   "source": [
    "Text classification is the process of classifying text strings or documents into different categories, depending\n",
    " upon the content of the document. Detecting user sentiment from a tweet, classifying an email as spam or\n",
    " ham, automatic tagging of customer queries, or classifying news articles into different categories like Politics,\n",
    " Stock Market, Sports, etc. are some of the real world applications of text classification.\n",
    " We can complete this task with the use of Natural Language Processing (NLP) and classification algorithms.\n",
    " NLP enables computers to understand and interpret human languages.\n",
    " In this lab, you will perform a simple text classification task: classifying movie reviews as either positive or\n",
    " negative based on their content. You will use the movie\n",
    " reviews dataset, which consists of two categories:\n",
    " pos for positive reviews and neg for negative reviews. Each document in the dataset is a movie review written\n",
    " in plain text, and the goal is to train a classifier that can automatically predict the sentiment of a given review\n",
    " as positive or negative. This dataset is commonly used as a benchmark for sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051fa71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 847.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn in e:\\my projects\\python\\co544 machine learnning\\venv\\lib\\site-packages (1.6.1)\n",
      "Collecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: joblib in e:\\my projects\\python\\co544 machine learnning\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "     ------------------------------------ 274.1/274.1 KB 845.7 kB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 KB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.5 in e:\\my projects\\python\\co544 machine learnning\\venv\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\my projects\\python\\co544 machine learnning\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\my projects\\python\\co544 machine learnning\\venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: colorama in e:\\my projects\\python\\co544 machine learnning\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'E:\\My Projects\\python\\CO544 Machine Learnning\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34fc911",
   "metadata": {},
   "source": [
    "##  2. Text Classification\n",
    "(a) Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2166d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # Regular expressions\n",
    "from sklearn.datasets import load_files # For loading dataset folders\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.corpus import stopwords # Stop words\n",
    "from nltk.stem import WordNetLemmatizer # Lemmatization\n",
    "# Download required NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161eb8e",
   "metadata": {},
   "source": [
    "(b) Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73008fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer (needed for later)\n",
    "lemmatizer = WordNetLemmatizer() # this is used to reduce words to their base form``\n",
    "# movie_data = load_files(r\"txt_sentoken\")\n",
    "movie_data = load_files(r\"movie_reviews\")\n",
    "X , y = movie_data.data, movie_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef14d6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2000\n",
      "Number of labels: 2000\n",
      "Target names (classes): ['neg', 'pos']\n",
      "\n",
      "First document (raw bytes):\n",
      "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so cal\"\n",
      "\n",
      "First document (decoded):\n",
      "arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \n",
      "it's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \n",
      "once again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \n",
      "in this so cal\n",
      "\n",
      "Label of first document: 0\n"
     ]
    }
   ],
   "source": [
    "# Show basic summary information\n",
    "print(f\"Number of documents: {len(X)}\")\n",
    "print(f\"Number of labels: {len(y)}\")\n",
    "print(f\"Target names (classes): {movie_data.target_names}\")\n",
    "# Show a sample file (before decoding)\n",
    "print(\"\\nFirst document (raw bytes):\")\n",
    "print(X[0][:500]) # show first 500 bytes\n",
    "# Decode and print a preview\n",
    "print(\"\\nFirst document (decoded):\")\n",
    "print(X[0].decode(\"utf-8\")[:500]) # show first 500 characters\n",
    "# Check label of first document\n",
    "print(f\"\\nLabel of first document: {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471013d",
   "metadata": {},
   "source": [
    "(c)Datapreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e24c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(len(X)):\n",
    " # 1. Decode from bytes to string\n",
    " document = X[i].decode(\"utf-8\")\n",
    " # # you can add a small check as follows.\n",
    " # print(X[0]) # Before decoding (bytes)\n",
    " # print(X[0].decode(\"utf-8\")) # After decoding\n",
    " # 2. Apply your regex substitutions\n",
    " document = re.sub(r\"\\W\", \" \", document) # remove special characters\n",
    " document = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", document) # single chars at beginning\n",
    " document = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", document) # single chars in middle\n",
    " document = re.sub(r\"\\d+\", \"\", document) # remove numbers\n",
    " document = re.sub(r\"\\s+\", \" \", document, flags=re.I) # multiple spaces to one\n",
    " # 3. Lowercase\n",
    " document = document.lower()\n",
    " # 4. Tokenize\n",
    " document = document.split()\n",
    " # 5. Lemmatize\n",
    " document = [lemmatizer.lemmatize(word) for word in document]\n",
    " # 6. Rejoin tokens if needed (optional)\n",
    " document = \" \".join(document)\n",
    " # 7. Append to new list\n",
    " documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70fd67",
   "metadata": {},
   "source": [
    "TASK1:Find data preprocessing steps other than mentioned above:\n",
    "\n",
    "- Remove stop words (later handled by CountVectorizer).\n",
    "\n",
    "- Spell correction using libraries like TextBlob.\n",
    "\n",
    "- Stemming (alternative to lemmatization).\n",
    "\n",
    "- Removing rare words (low frequency).\n",
    "\n",
    "- Synonym replacement to reduce feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e65fb",
   "metadata": {},
   "source": [
    "(d)Converttext intonumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89bdc5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1500)\n",
      "['ability' 'able' 'absolutely' ... 'york' 'young' 'younger']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1500,\n",
    "    min_df=7,\n",
    "    max_df=0.8,\n",
    "    stop_words=stopwords.words(\"english\")\n",
    ")\n",
    "X_vectors = vectorizer.fit_transform(documents).toarray()\n",
    "# To check the shape and vocabulary:\n",
    "print(X_vectors.shape) # (number_of_documents, number_of_features)\n",
    "print(vectorizer.get_feature_names_out()) # List of feature words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f104d22",
   "metadata": {},
   "source": [
    "TASK2:Discuss advantages and disadvantages of the Bag of Words model.\n",
    "| **Advantages**                           | **Disadvantages**                                     |\n",
    "| ---------------------------------------- | ----------------------------------------------------- |\n",
    "| Simple and easy to implement             | Ignores word order and context                        |\n",
    "| Works well for basic text classification | Large sparse feature vectors                          |\n",
    "| Efficient for small-medium datasets      | Cannot handle polysemy (same word, different meaning) |\n",
    "| Easy to interpret features               | Fails to capture semantic relationships               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb28c1",
   "metadata": {},
   "source": [
    "(e)TextClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7267a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.2,\n",
    "    random_state=0)\n",
    "# Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "predictions = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c274e",
   "metadata": {},
   "source": [
    "(f)Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d67ed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[164  44]\n",
      " [ 28 164]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       208\n",
      "           1       0.79      0.85      0.82       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "\n",
      "Accuracy:\n",
      "0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"\\nAccuracy:\")\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf76ea5",
   "metadata": {},
   "source": [
    "TASK3:TrainaRandomForestmodel,aSupportVectorMachinemodelandaNaiveBayesianclassifier.\n",
    "Comparetheaccuraciesandotherperformancemeasures(precision, recall,F1-score,confusionmatrix)ofall\n",
    "fourmodels includingtheLogisticRegressionmodel.What isthebestmodel?Justifyyouranswerbasedon\n",
    "thesemeasures.\n",
    "Note: It is importanttoevaluateclassificationmodelsusingmultipleperformancemeasures, suchaspreci\n",
    "sion, recall,F1-score,andconfusionmatrix,asaccuracyalonemaynotprovideenoughinsight,especiallyfor\n",
    "imbalanceddatasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359f401",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b43944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "[[164  44]\n",
      " [ 28 164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       208\n",
      "           1       0.79      0.85      0.82       192\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(confusion_matrix(y_test, log_pred))\n",
    "print(classification_report(y_test, log_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, log_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aad58b",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60046307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "[[168  40]\n",
      " [ 29 163]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       208\n",
      "           1       0.80      0.85      0.83       192\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n",
      "Accuracy: 0.8275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(confusion_matrix(y_test, rf_pred))\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45aba7",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02383672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Results:\n",
      "[[165  43]\n",
      " [ 33 159]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       208\n",
      "           1       0.79      0.83      0.81       192\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.81      0.81      0.81       400\n",
      "\n",
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=0)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "print(\"SVM Results:\")\n",
    "print(confusion_matrix(y_test, svm_pred))\n",
    "print(classification_report(y_test, svm_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, svm_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76f4a1",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5df1c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "[[166  42]\n",
      " [ 32 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       208\n",
      "           1       0.79      0.83      0.81       192\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.82      0.82      0.81       400\n",
      "weighted avg       0.82      0.81      0.82       400\n",
      "\n",
      "Accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(confusion_matrix(y_test, nb_pred))\n",
    "print(classification_report(y_test, nb_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, nb_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d14b80",
   "metadata": {},
   "source": [
    "After running the above code, compare:\n",
    "\n",
    "- Confusion matrix → How many true positives/negatives\n",
    "\n",
    "- Precision → Accuracy on positive class\n",
    "\n",
    "- Recall → Ability to detect positives\n",
    "\n",
    "- F1-score → Balance of precision/recall\n",
    "\n",
    "- Accuracy → Overall correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabb987",
   "metadata": {},
   "source": [
    "| Model               | Accuracy        | Precision    | Recall               | F1-score | Notes                    |\n",
    "| ------------------- | --------------- | ------------ | -------------------- | -------- | ------------------------ |\n",
    "| Logistic Regression | Good baseline | Balanced     | Balanced             | Balanced | Fast and simple          |\n",
    "| Random Forest       | Usually higher  | Robust       | Good generalization  | Strong   | May overfit              |\n",
    "| SVM                 | High accuracy   | High         | High                 | High     | Slower but precise       |\n",
    "| Naive Bayes         | Fastest         | May be lower | Good with small data | Fast     | Simple but less accurate |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d2403",
   "metadata": {},
   "source": [
    "Best Model Suggestion: In general, SVM often shows the best precision/recall trade-off on text classification, but you should justify with your actual output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
